{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers # L2규제\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # data augmentation\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler # callbacks 설정\n",
    "\n",
    "from tqdm.notebook import tqdm # 모델학습 진행 시간 파악\n",
    "import random # random seed를 뽑을때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#gpu사용\n",
    "config= tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 13\n",
    "img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
    "digit = train.loc[idx, 'digit']\n",
    "letter = train.loc[idx, 'letter']\n",
    "\n",
    "plt.title('Index: %i, Digit: %s, Letter: %s'%(idx, digit, letter))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['id', 'digit', 'letter'], axis=1).values\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_edit=x_train\n",
    "x_train = x_train/255\n",
    "\n",
    "y = train['digit']\n",
    "y_train = np.zeros((len(y), len(y.unique())))\n",
    "for i, digit in enumerate(y):\n",
    "    y_train[i, digit] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold=140 #임계값\n",
    "for i in range(0,2048):\n",
    "    for j in range(0,28):\n",
    "        for k in range(0,28):\n",
    "            if(x_edit[i][j][k][0]<=threshold):\n",
    "                x_edit[i][j][k][0]=0\n",
    "            else:\n",
    "                x_edit[i][j][k][0]=255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0.00784314]\n",
      " [0.        ]\n",
      " [0.02352941]\n",
      " [0.07058824]\n",
      " [0.07843137]\n",
      " [0.23921569]\n",
      " [0.26666667]\n",
      " [0.43921569]\n",
      " [0.83529412]\n",
      " [0.89803922]\n",
      " [0.92941176]\n",
      " [0.92156863]\n",
      " [0.91764706]\n",
      " [0.92156863]\n",
      " [0.88627451]\n",
      " [0.88627451]\n",
      " [0.94117647]\n",
      " [0.95294118]\n",
      " [0.93333333]\n",
      " [0.70980392]\n",
      " [0.21960784]\n",
      " [0.        ]\n",
      " [0.01568627]\n",
      " [0.00784314]\n",
      " [0.01176471]\n",
      " [0.        ]\n",
      " [0.00392157]\n",
      " [0.00784314]]\n"
     ]
    }
   ],
   "source": [
    "#결과확인\n",
    "print(x_edit[2046][7])\n",
    "print(x_train[2046][7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = np.repeat(x_train, 3, axis=1)\n",
    "x_train2 = np.repeat(x_train1, 3, axis=2)\n",
    "x_edit1=np.repeat(x_edit, 3, axis=1)\n",
    "x_edit2=np.repeat(x_edit1, 3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 84, 84, 1) (2048, 84, 84, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train2.shape, x_edit2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    inputs = Input(shape = (84,84,1))\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(inputs)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=5, strides=1, padding='same', activation='relu')(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(64, kernel_size=2, strides=1, padding='same', activation='relu')(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "    \n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=5, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(128, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    do=tf.keras.layers.Dropout(0.4)(pool)\n",
    "    \n",
    "    bn = tf.keras.layers.BatchNormalization()(do)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    bn = tf.keras.layers.BatchNormalization()(conv)\n",
    "    conv = tf.keras.layers.Conv2D(256, kernel_size=2, strides=1, padding='same', activation='relu',kernel_regularizer = regularizers.l2(0.001))(bn)\n",
    "    pool = tf.keras.layers.MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "    flatten = tf.keras.layers.Flatten()(pool)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(flatten)\n",
    "    dense = tf.keras.layers.Dense(1000, activation='relu')(bn)\n",
    "\n",
    "    bn = tf.keras.layers.BatchNormalization()(dense)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax')(bn)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 84, 84, 1)]       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 84, 84, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 84, 84, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 84, 84, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 84, 84, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 42, 42, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 42, 42, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 42, 42, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 42, 42, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 128)       65664     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 21, 21, 256)       131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 21, 21, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 21, 21, 256)       262400    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 25600)             102400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1000)              25601000  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 26,402,406\n",
      "Trainable params: 26,347,924\n",
      "Non-trainable params: 54,482\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_cnn_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 1/64 [..............................] - ETA: 0s - loss: 3.7677 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0140s vs `on_train_batch_end` time: 0.0239s). Check your callbacks.\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 3.7487 - accuracy: 0.3091\n",
      "Epoch 2/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.5742 - accuracy: 0.3721\n",
      "Epoch 3/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.3572 - accuracy: 0.4434\n",
      "Epoch 4/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.2602 - accuracy: 0.4595\n",
      "Epoch 5/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.1791 - accuracy: 0.4902\n",
      "Epoch 6/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.0462 - accuracy: 0.5068\n",
      "Epoch 7/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 2.0188 - accuracy: 0.5181\n",
      "Epoch 8/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.9291 - accuracy: 0.5264\n",
      "Epoch 9/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.8786 - accuracy: 0.5493\n",
      "Epoch 10/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.8495 - accuracy: 0.5542\n",
      "Epoch 11/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.8047 - accuracy: 0.5630\n",
      "Epoch 12/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.7211 - accuracy: 0.5835\n",
      "Epoch 13/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.6972 - accuracy: 0.6016\n",
      "Epoch 14/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.6749 - accuracy: 0.5874\n",
      "Epoch 15/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.6334 - accuracy: 0.6172\n",
      "Epoch 16/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.6322 - accuracy: 0.6079\n",
      "Epoch 17/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.5675 - accuracy: 0.6162\n",
      "Epoch 18/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.5005 - accuracy: 0.6382\n",
      "Epoch 19/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.4934 - accuracy: 0.6406\n",
      "Epoch 20/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.5129 - accuracy: 0.6465\n",
      "Epoch 21/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.4286 - accuracy: 0.6562\n",
      "Epoch 22/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.3922 - accuracy: 0.6714\n",
      "Epoch 23/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.3587 - accuracy: 0.6738\n",
      "Epoch 24/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.3363 - accuracy: 0.6860\n",
      "Epoch 25/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.3018 - accuracy: 0.6885\n",
      "Epoch 26/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.2847 - accuracy: 0.6987\n",
      "Epoch 27/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.2687 - accuracy: 0.6973\n",
      "Epoch 28/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.2554 - accuracy: 0.6992\n",
      "Epoch 29/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.2277 - accuracy: 0.7056\n",
      "Epoch 30/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.2128 - accuracy: 0.7148\n",
      "Epoch 31/200\n",
      "64/64 [==============================] - 2s 38ms/step - loss: 1.1910 - accuracy: 0.7178\n",
      "Epoch 32/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.1457 - accuracy: 0.7407\n",
      "Epoch 33/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.1366 - accuracy: 0.7373\n",
      "Epoch 34/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.1505 - accuracy: 0.7261\n",
      "Epoch 35/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 1.0894 - accuracy: 0.7461\n",
      "Epoch 36/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0925 - accuracy: 0.7485\n",
      "Epoch 37/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0478 - accuracy: 0.7666\n",
      "Epoch 38/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0517 - accuracy: 0.7637\n",
      "Epoch 39/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0520 - accuracy: 0.7632\n",
      "Epoch 40/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.0038 - accuracy: 0.7773\n",
      "Epoch 41/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 1.0020 - accuracy: 0.7822\n",
      "Epoch 42/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9779 - accuracy: 0.7886\n",
      "Epoch 43/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9521 - accuracy: 0.7896\n",
      "Epoch 44/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9181 - accuracy: 0.8071\n",
      "Epoch 45/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9259 - accuracy: 0.8042\n",
      "Epoch 46/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9186 - accuracy: 0.7949\n",
      "Epoch 47/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9120 - accuracy: 0.7988\n",
      "Epoch 48/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8926 - accuracy: 0.8115\n",
      "Epoch 49/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8745 - accuracy: 0.8203\n",
      "Epoch 50/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8579 - accuracy: 0.8369\n",
      "Epoch 51/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8423 - accuracy: 0.8286\n",
      "Epoch 52/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8152 - accuracy: 0.8408\n",
      "Epoch 53/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8188 - accuracy: 0.8452\n",
      "Epoch 54/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8102 - accuracy: 0.8408\n",
      "Epoch 55/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8216 - accuracy: 0.8335\n",
      "Epoch 56/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.8053 - accuracy: 0.8442\n",
      "Epoch 57/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7713 - accuracy: 0.8501\n",
      "Epoch 58/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7803 - accuracy: 0.8555\n",
      "Epoch 59/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7714 - accuracy: 0.8506\n",
      "Epoch 60/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7673 - accuracy: 0.8574\n",
      "Epoch 61/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7455 - accuracy: 0.8652\n",
      "Epoch 62/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7417 - accuracy: 0.8569\n",
      "Epoch 63/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7420 - accuracy: 0.8623\n",
      "Epoch 64/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7418 - accuracy: 0.8608\n",
      "Epoch 65/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7297 - accuracy: 0.8604\n",
      "Epoch 66/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.7337 - accuracy: 0.8701\n",
      "Epoch 67/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.7174 - accuracy: 0.8745\n",
      "Epoch 68/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6910 - accuracy: 0.8813\n",
      "Epoch 69/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6975 - accuracy: 0.8809\n",
      "Epoch 70/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6992 - accuracy: 0.8774\n",
      "Epoch 71/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6941 - accuracy: 0.8765\n",
      "Epoch 72/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6701 - accuracy: 0.8960\n",
      "Epoch 73/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6962 - accuracy: 0.8877\n",
      "Epoch 74/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6751 - accuracy: 0.8784\n",
      "Epoch 75/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6649 - accuracy: 0.8906\n",
      "Epoch 76/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6506 - accuracy: 0.8857\n",
      "Epoch 77/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6631 - accuracy: 0.8926\n",
      "Epoch 78/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6622 - accuracy: 0.8853\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6670 - accuracy: 0.8892\n",
      "Epoch 80/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6363 - accuracy: 0.8936\n",
      "Epoch 81/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6581 - accuracy: 0.8945\n",
      "Epoch 82/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6423 - accuracy: 0.8984\n",
      "Epoch 83/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6407 - accuracy: 0.8979\n",
      "Epoch 84/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6349 - accuracy: 0.9028\n",
      "Epoch 85/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6489 - accuracy: 0.8882\n",
      "Epoch 86/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6418 - accuracy: 0.8984\n",
      "Epoch 87/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6402 - accuracy: 0.8945\n",
      "Epoch 88/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6221 - accuracy: 0.8979\n",
      "Epoch 89/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6275 - accuracy: 0.9038\n",
      "Epoch 90/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6344 - accuracy: 0.8970\n",
      "Epoch 91/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6376 - accuracy: 0.8994\n",
      "Epoch 92/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6402 - accuracy: 0.8979\n",
      "Epoch 93/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6348 - accuracy: 0.8940\n",
      "Epoch 94/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6375 - accuracy: 0.9009\n",
      "Epoch 95/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6109 - accuracy: 0.9053\n",
      "Epoch 96/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6196 - accuracy: 0.8989\n",
      "Epoch 97/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6153 - accuracy: 0.9097\n",
      "Epoch 98/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6339 - accuracy: 0.8960\n",
      "Epoch 99/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6334 - accuracy: 0.8975\n",
      "Epoch 100/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6163 - accuracy: 0.9048\n",
      "Epoch 101/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6013 - accuracy: 0.9082\n",
      "Epoch 102/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6076 - accuracy: 0.9121\n",
      "Epoch 103/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6126 - accuracy: 0.9004\n",
      "Epoch 104/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5994 - accuracy: 0.9175\n",
      "Epoch 105/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6142 - accuracy: 0.9033\n",
      "Epoch 106/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6053 - accuracy: 0.9111\n",
      "Epoch 107/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6111 - accuracy: 0.9092\n",
      "Epoch 108/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6132 - accuracy: 0.9077\n",
      "Epoch 109/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6024 - accuracy: 0.9111\n",
      "Epoch 110/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6148 - accuracy: 0.9072\n",
      "Epoch 111/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5880 - accuracy: 0.9189\n",
      "Epoch 112/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5992 - accuracy: 0.9092\n",
      "Epoch 113/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6139 - accuracy: 0.9014\n",
      "Epoch 114/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6089 - accuracy: 0.9062\n",
      "Epoch 115/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6010 - accuracy: 0.9087\n",
      "Epoch 116/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5966 - accuracy: 0.9160\n",
      "Epoch 117/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.6150 - accuracy: 0.9048\n",
      "Epoch 118/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5885 - accuracy: 0.9219\n",
      "Epoch 119/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6107 - accuracy: 0.9062\n",
      "Epoch 120/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6107 - accuracy: 0.9058\n",
      "Epoch 121/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5904 - accuracy: 0.9126\n",
      "Epoch 122/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6115 - accuracy: 0.9048\n",
      "Epoch 123/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6204 - accuracy: 0.9023\n",
      "Epoch 124/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.6008 - accuracy: 0.9116\n",
      "Epoch 125/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5951 - accuracy: 0.9150\n",
      "Epoch 126/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6007 - accuracy: 0.9126\n",
      "Epoch 127/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5941 - accuracy: 0.9097\n",
      "Epoch 128/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5823 - accuracy: 0.9175\n",
      "Epoch 129/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5824 - accuracy: 0.9160\n",
      "Epoch 130/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5792 - accuracy: 0.9194\n",
      "Epoch 131/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5853 - accuracy: 0.9082\n",
      "Epoch 132/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6110 - accuracy: 0.9062\n",
      "Epoch 133/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5742 - accuracy: 0.9199\n",
      "Epoch 134/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5851 - accuracy: 0.9214\n",
      "Epoch 135/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5898 - accuracy: 0.9038\n",
      "Epoch 136/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6137 - accuracy: 0.9097\n",
      "Epoch 137/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5793 - accuracy: 0.9219\n",
      "Epoch 138/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6112 - accuracy: 0.9014\n",
      "Epoch 139/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6001 - accuracy: 0.9102\n",
      "Epoch 140/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5804 - accuracy: 0.9209\n",
      "Epoch 141/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5951 - accuracy: 0.9131\n",
      "Epoch 142/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5919 - accuracy: 0.9116\n",
      "Epoch 143/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6073 - accuracy: 0.9038\n",
      "Epoch 144/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5953 - accuracy: 0.9150\n",
      "Epoch 145/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6053 - accuracy: 0.9028\n",
      "Epoch 146/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5989 - accuracy: 0.9175\n",
      "Epoch 147/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5886 - accuracy: 0.9155\n",
      "Epoch 148/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5869 - accuracy: 0.9160\n",
      "Epoch 149/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5922 - accuracy: 0.9053\n",
      "Epoch 150/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5865 - accuracy: 0.9194\n",
      "Epoch 151/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5863 - accuracy: 0.9189\n",
      "Epoch 152/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5709 - accuracy: 0.9263\n",
      "Epoch 153/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5993 - accuracy: 0.9067\n",
      "Epoch 154/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5942 - accuracy: 0.9082\n",
      "Epoch 155/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6008 - accuracy: 0.9106\n",
      "Epoch 156/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5806 - accuracy: 0.9185\n",
      "Epoch 157/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5929 - accuracy: 0.9111\n",
      "Epoch 158/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5945 - accuracy: 0.9131\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6015 - accuracy: 0.9043\n",
      "Epoch 160/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5864 - accuracy: 0.9185\n",
      "Epoch 161/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6011 - accuracy: 0.9058\n",
      "Epoch 162/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5752 - accuracy: 0.9243\n",
      "Epoch 163/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5948 - accuracy: 0.9146\n",
      "Epoch 164/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5746 - accuracy: 0.9263\n",
      "Epoch 165/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5934 - accuracy: 0.9233\n",
      "Epoch 166/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6147 - accuracy: 0.9019\n",
      "Epoch 167/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6038 - accuracy: 0.9023\n",
      "Epoch 168/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6212 - accuracy: 0.9062\n",
      "Epoch 169/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6019 - accuracy: 0.9053\n",
      "Epoch 170/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.6119 - accuracy: 0.9082\n",
      "Epoch 171/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5962 - accuracy: 0.9116\n",
      "Epoch 172/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5992 - accuracy: 0.9180\n",
      "Epoch 173/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6038 - accuracy: 0.9126\n",
      "Epoch 174/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5945 - accuracy: 0.9155\n",
      "Epoch 175/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5987 - accuracy: 0.9131\n",
      "Epoch 176/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6043 - accuracy: 0.9028\n",
      "Epoch 177/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5772 - accuracy: 0.9180\n",
      "Epoch 178/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6029 - accuracy: 0.9106\n",
      "Epoch 179/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.6002 - accuracy: 0.8989\n",
      "Epoch 180/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5999 - accuracy: 0.9092\n",
      "Epoch 181/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5893 - accuracy: 0.9150\n",
      "Epoch 182/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5959 - accuracy: 0.9102\n",
      "Epoch 183/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5834 - accuracy: 0.9121\n",
      "Epoch 184/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5915 - accuracy: 0.9126\n",
      "Epoch 185/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5909 - accuracy: 0.9131\n",
      "Epoch 186/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5843 - accuracy: 0.9150\n",
      "Epoch 187/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6000 - accuracy: 0.9106\n",
      "Epoch 188/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6028 - accuracy: 0.9043\n",
      "Epoch 189/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5844 - accuracy: 0.9165\n",
      "Epoch 190/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6074 - accuracy: 0.8960\n",
      "Epoch 191/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6056 - accuracy: 0.9106\n",
      "Epoch 192/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5886 - accuracy: 0.9131\n",
      "Epoch 193/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5823 - accuracy: 0.9263\n",
      "Epoch 194/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5810 - accuracy: 0.9194\n",
      "Epoch 195/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6005 - accuracy: 0.9111\n",
      "Epoch 196/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6235 - accuracy: 0.9019\n",
      "Epoch 197/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5920 - accuracy: 0.9121\n",
      "Epoch 198/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5846 - accuracy: 0.9146\n",
      "Epoch 199/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.6083 - accuracy: 0.9053\n",
      "Epoch 200/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.5865 - accuracy: 0.9180\n"
     ]
    }
   ],
   "source": [
    "# 초반 이미지 학습\n",
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "# 이미지 증식 사용\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "model = create_cnn_model()\n",
    "epochs=200\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "history = model.fit(\n",
    "  datagen.flow(x_edit2, y_train, batch_size=32),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = x_edit2.shape[0]//32,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " 1/64 [..............................] - ETA: 0s - loss: 2.4758 - accuracy: 0.3438WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0130s vs `on_train_batch_end` time: 0.0219s). Check your callbacks.\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 1.9416 - accuracy: 0.5186\n",
      "Epoch 2/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 1.3361 - accuracy: 0.6782\n",
      "Epoch 3/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.9963 - accuracy: 0.7783\n",
      "Epoch 4/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.9233 - accuracy: 0.7979\n",
      "Epoch 5/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.8504 - accuracy: 0.8027\n",
      "Epoch 6/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.7535 - accuracy: 0.8369\n",
      "Epoch 7/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6798 - accuracy: 0.8550\n",
      "Epoch 8/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.6171 - accuracy: 0.8677\n",
      "Epoch 9/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.5597 - accuracy: 0.8916\n",
      "Epoch 10/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5487 - accuracy: 0.8892\n",
      "Epoch 11/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.5049 - accuracy: 0.8994\n",
      "Epoch 12/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4838 - accuracy: 0.9097\n",
      "Epoch 13/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.4419 - accuracy: 0.9233\n",
      "Epoch 14/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.4379 - accuracy: 0.9155\n",
      "Epoch 15/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.3977 - accuracy: 0.9282\n",
      "Epoch 16/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3618 - accuracy: 0.9448\n",
      "Epoch 17/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3559 - accuracy: 0.9390\n",
      "Epoch 18/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3585 - accuracy: 0.9351\n",
      "Epoch 19/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3349 - accuracy: 0.9512\n",
      "Epoch 20/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3280 - accuracy: 0.9448\n",
      "Epoch 21/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.3170 - accuracy: 0.9395\n",
      "Epoch 22/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.2926 - accuracy: 0.9531\n",
      "Epoch 23/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2706 - accuracy: 0.9639\n",
      "Epoch 24/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.2703 - accuracy: 0.9595\n",
      "Epoch 25/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2805 - accuracy: 0.9561\n",
      "Epoch 26/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2681 - accuracy: 0.9585\n",
      "Epoch 27/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.2507 - accuracy: 0.9629\n",
      "Epoch 28/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2384 - accuracy: 0.9634\n",
      "Epoch 29/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2493 - accuracy: 0.9653\n",
      "Epoch 30/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.2248 - accuracy: 0.9658\n",
      "Epoch 31/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2020 - accuracy: 0.9810\n",
      "Epoch 32/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.2122 - accuracy: 0.9717\n",
      "Epoch 33/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1862 - accuracy: 0.9854\n",
      "Epoch 34/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1942 - accuracy: 0.9766\n",
      "Epoch 35/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1839 - accuracy: 0.9785\n",
      "Epoch 36/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1844 - accuracy: 0.9766\n",
      "Epoch 37/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1696 - accuracy: 0.9819\n",
      "Epoch 38/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1583 - accuracy: 0.9863\n",
      "Epoch 39/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1761 - accuracy: 0.9756\n",
      "Epoch 40/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1577 - accuracy: 0.9854\n",
      "Epoch 41/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1622 - accuracy: 0.9790\n",
      "Epoch 42/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1609 - accuracy: 0.9839\n",
      "Epoch 43/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1590 - accuracy: 0.9785\n",
      "Epoch 44/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1540 - accuracy: 0.9849\n",
      "Epoch 45/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1470 - accuracy: 0.9858\n",
      "Epoch 46/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1382 - accuracy: 0.9873\n",
      "Epoch 47/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1350 - accuracy: 0.9893\n",
      "Epoch 48/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1292 - accuracy: 0.9912\n",
      "Epoch 49/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1312 - accuracy: 0.9883\n",
      "Epoch 50/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1333 - accuracy: 0.9897\n",
      "Epoch 51/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1285 - accuracy: 0.9893\n",
      "Epoch 52/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1255 - accuracy: 0.9917\n",
      "Epoch 53/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1239 - accuracy: 0.9907\n",
      "Epoch 54/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1183 - accuracy: 0.9912\n",
      "Epoch 55/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1201 - accuracy: 0.9907\n",
      "Epoch 56/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1226 - accuracy: 0.9897\n",
      "Epoch 57/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1271 - accuracy: 0.9858\n",
      "Epoch 58/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1156 - accuracy: 0.9927\n",
      "Epoch 59/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1111 - accuracy: 0.9927\n",
      "Epoch 60/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1179 - accuracy: 0.9912\n",
      "Epoch 61/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1146 - accuracy: 0.9893\n",
      "Epoch 62/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1050 - accuracy: 0.9941\n",
      "Epoch 63/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1103 - accuracy: 0.9922\n",
      "Epoch 64/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1078 - accuracy: 0.9907\n",
      "Epoch 65/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1062 - accuracy: 0.9907\n",
      "Epoch 66/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1001 - accuracy: 0.9941\n",
      "Epoch 67/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1003 - accuracy: 0.9951\n",
      "Epoch 68/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1037 - accuracy: 0.9941\n",
      "Epoch 69/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1002 - accuracy: 0.9941\n",
      "Epoch 70/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1030 - accuracy: 0.9941\n",
      "Epoch 71/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0968 - accuracy: 0.9956\n",
      "Epoch 72/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1030 - accuracy: 0.9937\n",
      "Epoch 73/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.1003 - accuracy: 0.9922\n",
      "Epoch 74/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0930 - accuracy: 0.9961\n",
      "Epoch 75/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0953 - accuracy: 0.9956\n",
      "Epoch 76/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0943 - accuracy: 0.9946\n",
      "Epoch 77/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0987 - accuracy: 0.9956\n",
      "Epoch 78/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.1030 - accuracy: 0.9927\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0969 - accuracy: 0.9956\n",
      "Epoch 80/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0936 - accuracy: 0.9927\n",
      "Epoch 81/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0920 - accuracy: 0.9961\n",
      "Epoch 82/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0961 - accuracy: 0.9932\n",
      "Epoch 83/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0920 - accuracy: 0.9941\n",
      "Epoch 84/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0942 - accuracy: 0.9937\n",
      "Epoch 85/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0868 - accuracy: 0.9971\n",
      "Epoch 86/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0899 - accuracy: 0.9961\n",
      "Epoch 87/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0890 - accuracy: 0.9951\n",
      "Epoch 88/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0931 - accuracy: 0.9937\n",
      "Epoch 89/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0889 - accuracy: 0.9966\n",
      "Epoch 90/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0898 - accuracy: 0.9951\n",
      "Epoch 91/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0907 - accuracy: 0.9941\n",
      "Epoch 92/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0902 - accuracy: 0.9956\n",
      "Epoch 93/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0872 - accuracy: 0.9956\n",
      "Epoch 94/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0840 - accuracy: 0.9971\n",
      "Epoch 95/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0861 - accuracy: 0.9961\n",
      "Epoch 96/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0826 - accuracy: 0.9990\n",
      "Epoch 97/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0856 - accuracy: 0.9961\n",
      "Epoch 98/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0877 - accuracy: 0.9956\n",
      "Epoch 99/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0848 - accuracy: 0.9961\n",
      "Epoch 100/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0884 - accuracy: 0.9961\n",
      "Epoch 101/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0826 - accuracy: 0.9980\n",
      "Epoch 102/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0893 - accuracy: 0.9956\n",
      "Epoch 103/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0855 - accuracy: 0.9956\n",
      "Epoch 104/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0909 - accuracy: 0.9961\n",
      "Epoch 105/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0806 - accuracy: 0.9971\n",
      "Epoch 106/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0883 - accuracy: 0.9956\n",
      "Epoch 107/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0878 - accuracy: 0.9951\n",
      "Epoch 108/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0811 - accuracy: 0.9976\n",
      "Epoch 109/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.0852 - accuracy: 0.9966\n",
      "Epoch 110/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0833 - accuracy: 0.9971\n",
      "Epoch 111/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0820 - accuracy: 0.9961\n",
      "Epoch 112/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0862 - accuracy: 0.9966\n",
      "Epoch 113/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0789 - accuracy: 0.9990\n",
      "Epoch 114/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0811 - accuracy: 0.9985\n",
      "Epoch 115/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0866 - accuracy: 0.9951\n",
      "Epoch 116/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0865 - accuracy: 0.9956\n",
      "Epoch 117/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0842 - accuracy: 0.9971\n",
      "Epoch 118/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.0827 - accuracy: 0.9971\n",
      "Epoch 119/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0826 - accuracy: 0.9961\n",
      "Epoch 120/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0817 - accuracy: 0.9971\n",
      "Epoch 121/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0811 - accuracy: 0.9971\n",
      "Epoch 122/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0845 - accuracy: 0.9956\n",
      "Epoch 123/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0888 - accuracy: 0.9941\n",
      "Epoch 124/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0812 - accuracy: 0.9966\n",
      "Epoch 125/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0856 - accuracy: 0.9961\n",
      "Epoch 126/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0807 - accuracy: 0.9976\n",
      "Epoch 127/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0807 - accuracy: 0.9971\n",
      "Epoch 128/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0817 - accuracy: 0.9985\n",
      "Epoch 129/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0799 - accuracy: 0.9976\n",
      "Epoch 130/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0845 - accuracy: 0.9956\n",
      "Epoch 131/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0827 - accuracy: 0.9976\n",
      "Epoch 132/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0847 - accuracy: 0.9961\n",
      "Epoch 133/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0847 - accuracy: 0.9956\n",
      "Epoch 134/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0834 - accuracy: 0.9976\n",
      "Epoch 135/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0875 - accuracy: 0.9956\n",
      "Epoch 136/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0876 - accuracy: 0.9946\n",
      "Epoch 137/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0791 - accuracy: 0.9980\n",
      "Epoch 138/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0805 - accuracy: 0.9961\n",
      "Epoch 139/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0852 - accuracy: 0.9932\n",
      "Epoch 140/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0799 - accuracy: 0.9976\n",
      "Epoch 141/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0880 - accuracy: 0.9941\n",
      "Epoch 142/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0846 - accuracy: 0.9961\n",
      "Epoch 143/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0840 - accuracy: 0.9971\n",
      "Epoch 144/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0825 - accuracy: 0.9971\n",
      "Epoch 145/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.0785 - accuracy: 0.9990\n",
      "Epoch 146/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0819 - accuracy: 0.9971\n",
      "Epoch 147/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0830 - accuracy: 0.9966\n",
      "Epoch 148/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0855 - accuracy: 0.9956\n",
      "Epoch 149/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0823 - accuracy: 0.9946\n",
      "Epoch 150/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0831 - accuracy: 0.9966\n",
      "Epoch 151/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0828 - accuracy: 0.9971\n",
      "Epoch 152/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0828 - accuracy: 0.9961\n",
      "Epoch 153/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0790 - accuracy: 0.9980\n",
      "Epoch 154/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0845 - accuracy: 0.9946\n",
      "Epoch 155/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0772 - accuracy: 0.9976\n",
      "Epoch 156/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0839 - accuracy: 0.9951\n",
      "Epoch 157/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0825 - accuracy: 0.9966\n",
      "Epoch 158/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0802 - accuracy: 0.9985\n",
      "Epoch 159/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0846 - accuracy: 0.9966\n",
      "Epoch 160/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0874 - accuracy: 0.9951\n",
      "Epoch 161/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0820 - accuracy: 0.9971\n",
      "Epoch 162/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0821 - accuracy: 0.9971\n",
      "Epoch 163/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0783 - accuracy: 0.9985\n",
      "Epoch 164/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0796 - accuracy: 0.9980\n",
      "Epoch 165/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.0826 - accuracy: 0.9966\n",
      "Epoch 166/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0874 - accuracy: 0.9951\n",
      "Epoch 167/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0867 - accuracy: 0.9956\n",
      "Epoch 168/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0841 - accuracy: 0.9951\n",
      "Epoch 169/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0821 - accuracy: 0.9976\n",
      "Epoch 170/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0816 - accuracy: 0.9966\n",
      "Epoch 171/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0818 - accuracy: 0.9966\n",
      "Epoch 172/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0883 - accuracy: 0.9961\n",
      "Epoch 173/200\n",
      "64/64 [==============================] - 2s 36ms/step - loss: 0.0811 - accuracy: 0.9961\n",
      "Epoch 174/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0808 - accuracy: 0.9966\n",
      "Epoch 175/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0804 - accuracy: 0.9966\n",
      "Epoch 176/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0828 - accuracy: 0.9946\n",
      "Epoch 177/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0915 - accuracy: 0.9922\n",
      "Epoch 178/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0836 - accuracy: 0.9966\n",
      "Epoch 179/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0837 - accuracy: 0.9966\n",
      "Epoch 180/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0823 - accuracy: 0.9976\n",
      "Epoch 181/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0818 - accuracy: 0.9966\n",
      "Epoch 182/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0821 - accuracy: 0.9966\n",
      "Epoch 183/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0811 - accuracy: 0.9980\n",
      "Epoch 184/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0842 - accuracy: 0.9966\n",
      "Epoch 185/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0818 - accuracy: 0.9966\n",
      "Epoch 186/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0862 - accuracy: 0.9946\n",
      "Epoch 187/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0840 - accuracy: 0.9956\n",
      "Epoch 188/200\n",
      "64/64 [==============================] - 2s 35ms/step - loss: 0.0862 - accuracy: 0.9941\n",
      "Epoch 189/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0789 - accuracy: 0.9980\n",
      "Epoch 190/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0786 - accuracy: 0.9985\n",
      "Epoch 191/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0794 - accuracy: 0.9990\n",
      "Epoch 192/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0803 - accuracy: 0.9980\n",
      "Epoch 193/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0791 - accuracy: 0.9995\n",
      "Epoch 194/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0827 - accuracy: 0.9971\n",
      "Epoch 195/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0820 - accuracy: 0.9956\n",
      "Epoch 196/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0825 - accuracy: 0.9966\n",
      "Epoch 197/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0849 - accuracy: 0.9971\n",
      "Epoch 198/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0798 - accuracy: 0.9980\n",
      "Epoch 199/200\n",
      "64/64 [==============================] - 2s 33ms/step - loss: 0.0850 - accuracy: 0.9951\n",
      "Epoch 200/200\n",
      "64/64 [==============================] - 2s 34ms/step - loss: 0.0792 - accuracy: 0.9976\n"
     ]
    }
   ],
   "source": [
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)\n",
    "\n",
    "# 이미지 증식 사용\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  \n",
    "    zoom_range = 0.1, \n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1)\n",
    "\n",
    "#model = create_cnn_model() #앞에거에 이어서 학습\n",
    "epochs=200\n",
    "\n",
    "#model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "history = model.fit(\n",
    "  datagen.flow(x_train2, y_train, batch_size=32),\n",
    "  epochs=epochs, \n",
    "  steps_per_epoch = x_train2.shape[0]//32,\n",
    "  callbacks=[annealer], \n",
    "  verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(['id', 'letter'], axis=1).values\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "x_t_edit=x_test\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = np.repeat(x_test, 3, axis=1)\n",
    "x_test2 = np.repeat(x_test1, 3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2054</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2055</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2056</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2057</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2060</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2061</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2062</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2064</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2066</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2068</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2069</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2070</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2071</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2072</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2073</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2074</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2076</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2077</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2079</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2080</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2082</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2084</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2086</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2089</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2090</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2092</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2096</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2098</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  digit\n",
       "0   2049      6\n",
       "1   2050      9\n",
       "2   2051      8\n",
       "3   2052      0\n",
       "4   2053      3\n",
       "5   2054      7\n",
       "6   2055      5\n",
       "7   2056      3\n",
       "8   2057      4\n",
       "9   2058      4\n",
       "10  2059      1\n",
       "11  2060      5\n",
       "12  2061      7\n",
       "13  2062      5\n",
       "14  2063      1\n",
       "15  2064      8\n",
       "16  2065      1\n",
       "17  2066      6\n",
       "18  2067      1\n",
       "19  2068      4\n",
       "20  2069      5\n",
       "21  2070      9\n",
       "22  2071      7\n",
       "23  2072      8\n",
       "24  2073      3\n",
       "25  2074      7\n",
       "26  2075      0\n",
       "27  2076      1\n",
       "28  2077      9\n",
       "29  2078      2\n",
       "30  2079      8\n",
       "31  2080      8\n",
       "32  2081      0\n",
       "33  2082      6\n",
       "34  2083      0\n",
       "35  2084      4\n",
       "36  2085      1\n",
       "37  2086      5\n",
       "38  2087      2\n",
       "39  2088      4\n",
       "40  2089      2\n",
       "41  2090      1\n",
       "42  2091      8\n",
       "43  2092      2\n",
       "44  2093      3\n",
       "45  2094      0\n",
       "46  2095      7\n",
       "47  2096      6\n",
       "48  2097      2\n",
       "49  2098      8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('data/submission.csv')\n",
    "submission['digit'] = np.argmax(model.predict(x_test2), axis=1)\n",
    "submission.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('batchsize 둘다32.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "threshold=140 #임계값\n",
    "for i in range(0,20480):\n",
    "    if(i%3000==0):\n",
    "        print(i)\n",
    "    for j in range(0,28):\n",
    "        for k in range(0,28): \n",
    "            if(x_t_edit[i][j][k][0]<=threshold):\n",
    "                x_t_edit[i][j][k][0]=0\n",
    "            else:\n",
    "                x_t_edit[i][j][k][0]=255/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t_edit1 = np.repeat(x_t_edit, 3, axis=1)\n",
    "x_t_edit2 = np.repeat(x_t_edit1, 3, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2049</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2050</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2051</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2053</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2054</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2055</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2056</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2057</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2058</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2060</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2061</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2062</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2063</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2064</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2065</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2066</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2067</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2068</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2069</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2070</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2071</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2073</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2074</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2075</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2076</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2077</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2078</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2079</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2080</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2081</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2082</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2083</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2084</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2085</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2086</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2087</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2088</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2089</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2090</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2091</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2092</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2094</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2095</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2098</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  digit\n",
       "0   2049      2\n",
       "1   2050      8\n",
       "2   2051      2\n",
       "3   2052      8\n",
       "4   2053      8\n",
       "5   2054      8\n",
       "6   2055      8\n",
       "7   2056      2\n",
       "8   2057      2\n",
       "9   2058      8\n",
       "10  2059      2\n",
       "11  2060      8\n",
       "12  2061      2\n",
       "13  2062      8\n",
       "14  2063      5\n",
       "15  2064      8\n",
       "16  2065      2\n",
       "17  2066      2\n",
       "18  2067      5\n",
       "19  2068      2\n",
       "20  2069      3\n",
       "21  2070      8\n",
       "22  2071      8\n",
       "23  2072      2\n",
       "24  2073      3\n",
       "25  2074      8\n",
       "26  2075      8\n",
       "27  2076      9\n",
       "28  2077      8\n",
       "29  2078      2\n",
       "30  2079      2\n",
       "31  2080      2\n",
       "32  2081      8\n",
       "33  2082      2\n",
       "34  2083      2\n",
       "35  2084      8\n",
       "36  2085      2\n",
       "37  2086      2\n",
       "38  2087      2\n",
       "39  2088      8\n",
       "40  2089      2\n",
       "41  2090      2\n",
       "42  2091      8\n",
       "43  2092      8\n",
       "44  2093      3\n",
       "45  2094      2\n",
       "46  2095      9\n",
       "47  2096      8\n",
       "48  2097      2\n",
       "49  2098      8"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission2 = pd.read_csv('data/submission.csv')\n",
    "submission2['digit'] = np.argmax(model.predict(x_t_edit2), axis=1)\n",
    "submission2.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head만봐도 정확도가 너무 안좋아서 따로 저장 안하는게 나을듯?\n",
    "submission2.to_csv('두번 이어서 훈련_이진화otest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
